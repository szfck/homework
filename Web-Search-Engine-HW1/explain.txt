### Program entry

main.py read search type and search term from stdin, then call related crawl to search.

### Google Search result

use urllib to crawl the top 10 in google search as the start page

### Focused Crawl

use priority queue to store the promised url. Every time download page 
from url, find all the links in the page. Then calculate the promise score for 
each link based on anchor text (count the search terms in link anchor text).
The promise could also be updated, there are two ways:
1. This link also exists in another page, then add the promise in these two pages together
2. The page downloaded has relevance score, then link's promise in the page would also be influenced by the relevance value

I design a data structure which could update the promise value based on priority queue. For each link in the queue, I also recorded the latest promise update time in a dictionary. Every time I got the top of the priority queue, I will check the if the update time of link is the latest one, otherwise I will just throw it. 

### relevance score

I use cosine measure to calculate relevance score. But since I don't have the full collection when crawling, so the term frequent and document number is dynamic.

### harvest rate

Here is how I define the harvest rate: firstly, I calculate the average relevance score of start pages from google search. Then I calculate the threshhold of harvest rate = average relevance score of start pages * 0.5, the the page downloaded has higher relevance score than it, it is related with search terms 

### Crawl limit

- limit the number of same hostname: because some page like wili could have thousands of pages
- limit the depth of search: to avoid getting stuck under the same web page